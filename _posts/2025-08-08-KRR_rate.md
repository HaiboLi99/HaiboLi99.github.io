---
layout: post
title: Sums of Independent Random Self-adjoint Operators
date: 2025-08-09 
description: Operator Bernstein
giscus_comments: true
related_posts: false
toc:
  beginning: true
---



### 1. Basic setting
In statistical learning, random self-adjoint operators frequently emerge in the study of approximation errors. In such scenarios, the concentration inequalities for a sum of independent random self-adjoint operators are often useful for deriving sharp bounds. 

In our setting, assume $x$ be a random data in $$X$$. Rigorously speaking, let $$(\Omega,\mathcal{A},\mathbb{P})$$ be a probability space and $$x: (\Omega,\mathcal{A})\rightarrow (X,\mathcal{F})$$ be measurable, such that the distribution on $$X$$ is $$\nu=\mathbb{P}\circ x^{-1}$$. Let $K(\cdot,\cdot): X \times X \rightarrow \mathbb{R}$
be a symmetric positive definite kernel, satisfying $$\mathrm{sup}_{x\in X} K(x,x) \leq \kappa$$. Define the kernel integral operator $$T$$ on $$L^{2}(X,\mathcal{F},\nu)$$ be

$$
\begin{equation}
(Tf)(x) = \int_{X}K(x,y)f(y)\nu(\mathrm{d}y).
\end{equation}
$$

Then $T$ is a <a href="https://en.wikipedia.org/wiki/Hilbert%E2%80%93Schmidt_operator" target="_blank">Hilbert--Schmidt operator</a> with the Hilbert--Schmidt norm satisfying

$$
\begin{equation*}
\|T\|_{\mathrm{HS}}^{2}
= \int_{X\times X} |K(x,y)|^2 \, (\nu\otimes\nu)(\mathrm{d}(x,y))
\leq \kappa^2 ,
\end{equation*}
$$

where we have used $$K(x,y)^2\leq K(x,x)K(y,y)$$. Now suppose we have $$n$$ i.i.d. samples of $$x$$, i.e., $$\{x_1,\dots,x_n\}$$ are i.i.d. random variables with the same distribution as $$x$$. Define the empirical operator

$$
\begin{equation}
T_n = \frac{1}{n}\sum_{i=1}^{n} K_{x_i}\otimes K_{x_i} .
\end{equation}
$$

Note that $$T_i$$ is a random self-adjoint operator, and it will approximate $$T$$ as $$n\to \infty$$. 
We want to estimate the upper bound on $$\mathbb{E}_{\mathbb{P}}[\|T_n-T\|^2]=\mathbb{E}_{\nu^{\otimes n}}[\|T_n-T\|^2]$$, where $$\|\cdot\|$$ is the operator norm. Note that the first expectation is calcuated in $$\Omega$$ while the second is calculated in $$\Pi_{i=1}^{n}X$$.


### 2. Bernstein-type concentration inequalities
Let $$S_i=K_{x_i}\otimes K_{x_i}-T$$. Then $$\frac{1}{n}\sum_{i=1}^{n}S_i=T_n-T$$, and

$$
\begin{equation*}
\mathbb{E}[S_i] = \int_{X}K_{x}\otimes K_{x}\nu(\mathrm{d}x) - T = 0,
\end{equation*}
$$

where the <a href="https://en.wikipedia.org/wiki/Bochner_integral" target="_blank">Bochner integral</a> $$\int_{X}K_{x}\otimes K_{x}\nu(\mathrm{d}x)$$ converges in the $$\|\cdot\|_{\mathrm{HS}}$$ norm. We can deal with the sum of the i.i.d operators $$\{S_1,\cdots,S_n\}$$ with zero means. 

The following result is the standard <a href="https://en.wikipedia.org/wiki/Bernstein_inequalities_(probability_theory)" target="_blank">Bernstein inequality</a> for real random variables. It demonstrates that a sum of independent bounded random variables exhibits normal concentration near its mean on a scale determined by the variance of the sum.

<div class="theobox theorem" id="thm-bernstein" data-title="(Bernstein inequality)">
  <p>
    Let $$\{S_1, \dots, S_n\}$$ be independent and centered real random variables satisfying $$|S_i|\leq R$$ a.s. for $$i=1,\dots,n$$. Let $$Z=\sum_{i=1}^{n}S_i$$ and assume $$\mathbb{E}[Z^2]=\sum_{i=1}^{n}\mathbb{E}[S_i^2]\leq\sigma^2$$. Then for any $$t>0$$,
  </p>

  $$
  \mathbb{P}(|Z|\geq t) \leq \exp\!\left(\frac{-t^2/2}{\sigma^2 + Rt/3}\right).
  $$
  
</div>

Sums of independent random matrices exhibit the same type of behavior,
where the normal concentration depends on a matrix generalization of the variance
and the tails are controlled by a uniform bound on the spectral norm of each
summand. See [\[1\]](#ref-tropp2012).

<div class="theobox theorem" id="thm-bernstein" data-title="(Matrix Bernstein concentration)">
  <p>
    Let $$\{S_1, \dots, S_n\}$$ be independent random self-adjoint matrices with dimension $$d$$. Assume each random matrix satisfies 
  </p>

  $$
  \mathbb{E}[S_i] = \mathbf{0} \quad \mathrm{and} \quad
  \|S_i\| \leq R \ \ \mathrm{a.s.},
  $$

  where $$\|\cdot\|$$ is the matrix spectral norm, and $$\|\sum_{i=1}^{n}\mathbb{E}[S_i^2]\|\leq\sigma^2$$. Then for any $$t>0$$,

  $$
  \begin{equation}
  \mathbb{P}(\|Z\|\geq t) \leq d \cdot \exp\!\left(\frac{-t^2/2}{\sigma^2 + Rt/3}\right).
  \end{equation}
  $$

</div>




<div class="theobox theorem" id="thm-bernstein" data-title="(Operator Bernstein concentration)">
  <p>
    Let $$\{S_1, \dots, S_n\}$$ be independent random self-adjoint Hilbert–Schmidt operators, where $$S_i: H\rightarrow H$$ acts on a separable Hilbert space $$H$$. Assume each random operator satisfies 
  </p>

  $$
  \mathbb{E}[S_i] = \mathbf{0} \quad \mathrm{and} \quad
  \|S_i\| \leq R \ \ \mathrm{a.s.},
  $$

  where $$\|\cdot\|$$ is the operator norm, and $$\|\sum_{i=1}^{n}\mathbb{E}[S_i^2]\|\leq\sigma^2$$. For any Hilbert–Schmidt operator $$A$$ define $$r(A)=\frac{\mathrm{tr}(A)}{\|A\|}$$. Then for any $$t\geq \frac{1}{6}xx$$,

  $$
  \begin{equation}
  \mathbb{P}(\|Z\|\geq t) \leq 14r\left(\sum_{i=1}^{n}\mathbb{E}[S_i^2]\right) \exp\!\left(\frac{-t^2/2}{\sigma^2 + Rt/3}\right).
  \end{equation}
  $$

</div>









### 3. Mean square upper bound
xxx




### 4. High probability upper bound
There are several approaches to derive the probability flow ODE of Langevin dynamics. The most accessible one is using the result from [score-based diffusion models](https://yang-song.net/blog/2021/score/#probability-flow-ode). In score-based diffusion models, it is shown that each SDE has an associated probability flow ODE sharing the same marginal $$q_t$$ (also see the Eq. (13) in [Song et. al., 2021](https://arxiv.org/abs/2011.13456)). Simply using this result, we can convert the Langevin SDE to the associated ODE,
\begin{aligned}
\mathrm{d} \mathbf{x}\_t = \Big [\nabla\_\mathbf{x} \log p(\mathbf{x}\_t)- \nabla\_\mathbf{x} \log q_t(\mathbf{x}\_t)\Big]\mathrm{d}t.
\end{aligned}
The marginal laws of the probability flow ODE also follow the same Fokker-Planck equation. The probability flow ODE differs from Langevin dynamics only in the nature of particle evolution: the former is deterministic, while the latter is stochastic. Similarly, using the Euler method to discretize the ODE gives 
\begin{align}
\mathbf{x}\_{i+1} \leftarrow \mathbf{x}\_{i} + \epsilon \big[\nabla_{\mathbf{x}} \log {p(\mathbf{x}\_i)}- \nabla_{\mathbf{x}} \log {q_{i}({\mathbf{x}\_i})}\big], \quad i=0, 1, 2\cdots
\end{align}
It is worth noting the vector field of the probability flow ODE is the gradient of the log density ratio $$\log \big[p(\mathbf{x}_i) / q_{i}({\mathbf{x}_i})\big]$$. If we have access to the log density ratio, we can use the Euler method to simulate the ODE. Next, we will discuss a method to obtain the log density ratio, analogous to training the discriminator in GANs.




<!-- Referecnes -->
<h3 id="references">References</h3>
<ol>
  <li id="ref-tropp2012">
    Joel A. Tropp (2012). 
    <a href="https://doi.org/10.1007/s10208-011-9099-z" target="_blank">
      User-friendly tail bounds for sums of random matrices, 12, 389–434. 
      <em>Foundations of Computational Mathematics.</em>
    </a>
  </li>
  <li id="ref-Minsker2017">
    Stanislav Minsker (2017). 
    <a href="http://dx.doi.org/10.1016/j.spl.2017.03.020" target="_blank">
    On some extensions of Bernstein’s inequality for self-adjoint operators, 127, 111-119.
      <em>Statistics and Probability Letters.</em>
    </a>
  </li>
</ol>
