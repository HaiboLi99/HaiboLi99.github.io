---
layout: post
title: "Sums of Independent Random Self-adjoint Operators"
date: 2025-08-09
description: "Operator Bernstein"
giscus_comments: true
related_posts: false
markdown: 0
toc: xxx
beginning: true
# tags: [operator, concentration, bernstein]
---

\subsection*{1. Basic setting}
In statistical learning, random self-adjoint operators frequently emerge in the study of approximation errors. In such scenarios, the concentration inequalities for a sum of independent random self-adjoint operators are often useful for deriving sharp bounds.

In our setting, assume $x$ be a random data in $X$. Rigorously speaking, let $(\Omega,\mathcal{A},\mathbb{P})$ be a probability space and $x: (\Omega,\mathcal{A})\rightarrow (X,\mathcal{F})$ be measurable, such that the distribution on $X$ is $\nu=\mathbb{P}\circ x^{-1}$. Let $K(\cdot,\cdot): X \times X \rightarrow \mathbb{R}$
be a symmetric positive definite kernel, satisfying $\mathrm{sup}_{x\in X} K(x,x) \leq \kappa$. Define the kernel integral operator $T$ on $L^{2}(X,\mathcal{F},\nu)$ be
\begin{equation}
  (Tf)(x) = \int_{X}K(x,y)f(y)\,\nu(\mathrm{d}y). 
\end{equation}
Then $T$ is a \href{https://en.wikipedia.org/wiki/Hilbert%E2%80%93Schmidt_operator}{Hilbert--Schmidt operator} with the Hilbert--Schmidt norm satisfying
\[ \|T\|_{\mathrm{HS}}^{2}
= \int_{X\times X} |K(x,y)|^2 \, (\nu\otimes\nu)(\mathrm{d}(x,y))
\leq \kappa^2 ,\]
where we have used $K(x,y)^2\leq K(x,x)K(y,y)$. 

Now suppose we have $n$ i.i.d. samples of $x$, i.e., $\{x_1,\dots,x_n\}$ are i.i.d. random variables with the same distribution as $x$. Define the empirical operator
\begin{equation}
  T_n = \frac{1}{n}\sum_{i=1}^{n} K_{x_i}\otimes K_{x_i} .
\end{equation}
Note that $T_i$ is a random self-adjoint operator, and it will approximate $T$ as $n\to \infty$. 
We want to estimate the upper bound on $\mathbb{E}_{\mathbb{P}}[\|T_n-T\|^2]=\mathbb{E}_{\nu^{\otimes n}}[\|T_n-T\|^2]$, where $\|\cdot\|$ is the operator norm. Note that the first expectation is calculated in $\Omega$ while the second is calculated in $\Pi_{i=1}^{n}X$.

\subsection*{2. Bernstein-type concentration inequalities}
Let $S_i=K_{x_i}\otimes K_{x_i}-T$. Then $\frac{1}{n}\sum_{i=1}^{n}S_i=T_n-T$, and
\[
\mathbb{E}[S_i] = \int_{X}K_{x}\otimes K_{x}\,\nu(\mathrm{d}x) - T = 0,
\]
where the \href{https://en.wikipedia.org/wiki/Bochner_integral}{Bochner integral} $\int_{X}K_{x}\otimes K_{x}\,\nu(\mathrm{d}x)$ converges in the $\|\cdot\|_{\mathrm{HS}}$ norm. We can deal with the sum of the i.i.d operators $\{S_1,\dots,S_n\}$ with zero means. 

The following result is the standard \href{https://en.wikipedia.org/wiki/Bernstein_inequalities_(probability_theory)}{Bernstein inequality} for real random variables. It demonstrates that a sum of independent bounded random variables exhibits normal concentration near its mean on a scale determined by the variance of the sum.

\begin{theorem}[Bernstein inequality] 
  Let $\{S_1, \dots, S_n\}$ be independent and centered real random variables satisfying $|S_i|\leq R$ a.s. for $i=1,\dots,n$. Let $Z=\sum_{i=1}^{n}S_i$ and assume $\mathbb{E}[Z^2]=\sum_{i=1}^{n}\mathbb{E}[S_i^2]\leq\sigma^2$. Then for any $t>0$,
\[
\mathbb{P}(|Z|\geq t) \leq \exp\!\left(\frac{-t^2/2}{\sigma^2 + Rt/3}\right).
\]
\end{theorem}



Sums of independent random matrices exhibit the same type of behavior,
where the normal concentration depends on a matrix generalization of the variance
and the tails are controlled by a uniform bound on the spectral norm of each
summand. 
See \cite{tropp2012}.

\begin{theorem}[Matrix Bernstein concentration]
Let $\{S_1, \dots, S_n\}$ be independent random self-adjoint matrices with dimension $d$. Assume each random matrix satisfies 
\[
\mathbb{E}[S_i] = \mathbf{0} \quad \mathrm{and} \quad
\|S_i\| \leq R \ \ \mathrm{a.s.},
\]
where $\|\cdot\|$ is the matrix spectral norm, and $\|\sum_{i=1}^{n}\mathbb{E}[S_i^2]\|\leq\sigma^2$. Then for any $t>0$,
\[
\mathbb{P}(\|Z\|\geq t) \leq d \cdot \exp\!\left(\frac{-t^2/2}{\sigma^2 + Rt/3}\right).
\]
\end{theorem}

\begin{theorem} (Operator Bernstein concentration)
Let $\{S_1, \dots, S_n\}$ be independent random self-adjoint Hilbert--Schmidt operators, where $S_i: H\rightarrow H$ acts on a separable Hilbert space $H$. Assume each random operator satisfies 
\[
\mathbb{E}[S_i] = \mathbf{0} \quad \mathrm{and} \quad
\|S_i\| \leq R \ \ \mathrm{a.s.},
\]
where $\|\cdot\|$ is the operator norm, and $\|\sum_{i=1}^{n}\mathbb{E}[S_i^2]\|\leq\sigma^2$. For any Hilbert--Schmidt operator $A$ define $r(A)=\frac{\mathrm{tr}(A)}{\|A\|}$. Then for any $t\geq \frac{1}{6}xx$,
\[
\mathbb{P}(\|Z\|\geq t) \leq 14\,r\!\left(\sum_{i=1}^{n}\mathbb{E}[S_i^2]\right) \exp\!\left(\frac{-t^2/2}{\sigma^2 + Rt/3}\right).
\]
\end{theorem}


\cite{tropp2012,minsker2017}


\subsection*{3. Mean square upper bound}
xxx






\subsection*{4. High probability upper bound}
Simply using this result, we can convert the Langevin SDE to the associated ODE,
\[
\mathrm{d} \mathbf{x}_t = \left [\nabla_\mathbf{x} \log p(\mathbf{x}_t)- \nabla_\mathbf{x} \log q_t(\mathbf{x}_t)\right]\,\mathrm{d}t.
\]
The marginal laws of the probability flow ODE also follow the same Fokker--Planck equation. The probability flow ODE differs from Langevin dynamics only in the nature of particle evolution: the former is deterministic, while the latter is stochastic. 

Similarly, using the Euler method to discretize the ODE gives 
\[
\mathbf{x}_{i+1} \leftarrow \mathbf{x}_{i} + \epsilon \left[\nabla_{\mathbf{x}} \log {p(\mathbf{x}_i)}- \nabla_{\mathbf{x}} \log {q_{i}({\mathbf{x}_i})}\right], \quad i=0, 1, 2,\dots
\]
It is worth noting the vector field of the probability flow ODE is the gradient of the log density ratio $\log \big[p(\mathbf{x}_i) / q_{i}({\mathbf{x}_i})\big]$. If we have access to the log density ratio, we can use the Euler method to simulate the ODE. Next, we will discuss a method to obtain the log density ratio, analogous to training the discriminator in GANs.



%--------------------------
\subsection*{References}

